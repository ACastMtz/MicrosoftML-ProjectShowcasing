# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wQeRdRUKAXXbke5UPXz2j24L_2oNVKio

This project will use the Delhi city weather dataset in 2013 to 2017 to implement Time Series ML using LSTM. First we import Kaggle library
"""

from google.colab import files
files.upload() #upload kaggle.json

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json

"""Next we import data from Kaggle"""

!kaggle datasets download -d sumanthvrao/daily-climate-time-series-data

"""Next we unzip file"""

from zipfile import ZipFile
file_name = 'daily-climate-time-series-data.zip'
with ZipFile(file_name,'r')as zip:
  zip.extractall()

"""Next we import numpy, pandas, keras, matplotlib and tensor flow library"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

"""Then, convert the dataset to a dataframe with the read_csv () function. Display the top 5 data on the dataframe using the head () function."""

data_train = pd.read_csv('DailyDelhiClimateTrain.csv')
data_train.head()

"""Next we check if any values ​​are missing from the dataset using the isnull () function."""

data_train.isnull().sum()

"""To plot the data we use the plot function from the matplotlib library. It can be seen from the output cell that we run it shows that our data is a seasonal time series."""

dates = data_train['date'].values
temp  = data_train['meantemp'].values
 
 
plt.figure(figsize=(15,5))
plt.plot(dates, temp)
plt.title('Temperature average',
          fontsize=20);

"""Next write a function below that converts data into a format acceptable to the model. The function below accepts a series / attribute we converted to numpy type, then returns the labels and attributes from the dataset in batch form."""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

"""Furthermore, for the model architecture use 2 LSTM layers. When using 2 LSTM layers, note that the first layer must have a return_sequences parameter which is True."""

print(temp)
train_set = windowed_dataset(temp, window_size=60, batch_size=100, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

"""Then in the optimizer, we use the learning rate and momentum parameters as below. The loss function that can be tried for this is the Huber, which is one of the most commonly used loss functions in the case of time series. The metric used to evaluate the model is MAE."""

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,epochs=100)

"""Next we try to implement on data 2017-2019"""

data_train = pd.read_csv('DailyDelhiClimateTest.csv')
data_train.head()

temp  = data_train['meantemp'].values
print(temp)

train_set = windowed_dataset(temp, window_size=60, batch_size=100, shuffle_buffer=1000)

"""We see the different with actual value is very small"""

print(model.predict(train_set))